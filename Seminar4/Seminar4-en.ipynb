{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano, Lasagne\n",
    "and why they matter\n",
    "\n",
    "\n",
    "### got no lasagne?\n",
    "Install the __bleeding edge__ version from here: http://lasagne.readthedocs.org/en/latest/user/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warming up\n",
    "* Implement a function that computes the sum of squares of numbers from 0 to N\n",
    "* Use numpy or python\n",
    "* An array of numbers 0 to N - numpy.arange(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#def sum_squares(N):\n",
    "#    return <student.Implement_me()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sum_squares' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d1d25c93580a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'sum_squares(10**8)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'eval'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1163\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1164\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sum_squares' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sum_squares(10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# theano teaser\n",
    "\n",
    "Doing the very same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#I gonna be function parameter\n",
    "N = T.scalar(\"a dimension\",dtype='int32')\n",
    "\n",
    "\n",
    "#i am a recipe on how to produce sum of squares of arange of N given N\n",
    "result = (T.arange(N)**2).sum()\n",
    "\n",
    "#Compiling the recipe of computing \"result\" given N\n",
    "sum_function = theano.function(inputs = [N],outputs=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 781 ms, sys: 370 ms, total: 1.15 s\n",
      "Wall time: 1.16 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(662921401752298880)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sum_function(10**8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does it work?\n",
    "* 1 You define inputs f your future function;\n",
    "* 2 You write a recipe for some transformation of inputs;\n",
    "* 3 You compile it;\n",
    "* You have just got a function!\n",
    "* The gobbledegooky version: you define a function as symbolic computation graph.\n",
    "\n",
    "\n",
    "* There are two main kinвs of entities: \"Inputs\" and \"Transformations\"\n",
    "* Both can be numbers, vectors, matrices, tensors, etc.\n",
    "* Both can be integers, floats of booleans (uint8) of various size.\n",
    "\n",
    "\n",
    "* An input is a placeholder for function parameters.\n",
    " * N from example above\n",
    "\n",
    "\n",
    "* Transformations are the recipes for computing something given inputs and transformation\n",
    " * (T.arange(N)^2).sum() are 3 sequential transformations of N\n",
    " * Doubles all functions of numpy vector syntax\n",
    " * You can almost always go with replacing \"np.function\" with \"T.function\" aka \"theano.tensor.function\"\n",
    "   * np.mean -> T.mean\n",
    "   * np.arange -> T.arange\n",
    "   * np.cumsum -> T.cumsum\n",
    "   * and so on.\n",
    "   * builtin operations also work that way\n",
    "   * np.arange(10).mean() -> T.arange(10).mean()\n",
    "   * Once upon a blue moon the functions have different names or locations (e.g. T.extra_ops)\n",
    "     * Ask us or google it\n",
    " \n",
    " \n",
    "Still confused? We gonna fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Inputs\n",
    "example_input_integer = T.scalar(\"scalar input\",dtype='float32')\n",
    "\n",
    "example_input_tensor = T.tensor4(\"four dimensional tensor input\") #dtype = theano.config.floatX by default\n",
    "#не бойся, тензор нам не пригодится\n",
    "\n",
    "\n",
    "\n",
    "input_vector = T.vector(\"\", dtype='int32') # vector of integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transformations\n",
    "\n",
    "#transofrmation: elementwise multiplication\n",
    "double_the_vector = input_vector*2\n",
    "\n",
    "#elementwise cosine\n",
    "elementwise_cosine = T.cos(input_vector)\n",
    "\n",
    "#difference between squared vector and vector itself\n",
    "vector_squares = input_vector**2 - input_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Practice time:\n",
    "#create two vectors of size float32\n",
    "my_vector = student.init_float32_vector()\n",
    "my_vector2 = student.init_one_more_such_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Write a transformation(recipe):\n",
    "#(vec1)*(vec2) / (sin(vec1) +1)\n",
    "my_transformation = student.implementwhatwaswrittenabove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print my_transformation\n",
    "#it's okay it aint a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling\n",
    "* So far we were using \"symbolic\" variables and transformations\n",
    " * Defining the recipe for computation, but not computing anything\n",
    "* To use the recipe, one should compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = [<two vectors that my_transformation depends on>]\n",
    "outputs = [<What do we compute (can be a list of several transformation)>]\n",
    "\n",
    "# The next lines compile a function that takes two vectors and computes your transformation\n",
    "my_function = theano.function(\n",
    "    inputs,outputs,\n",
    "    allow_input_downcast=True #automatic type casting for input parameters (e.g. float64 -> float32)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#using function with, lists:\n",
    "print \"using python lists:\"\n",
    "print my_function([1,2,3],[4,5,6])\n",
    "print\n",
    "\n",
    "#Or using numpy arrays:\n",
    "#btw, that 'float' dtype is casted to secong parameter dtype which is float32\n",
    "print \"using numpy arrays:\"\n",
    "print my_function(np.arange(10),\n",
    "                  np.linspace(5,6,10,dtype='float'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging\n",
    "* Compilation can take a while for big functions\n",
    "* To avoid waiting, one can evaluate transformations without compiling\n",
    "* Without compilation, the code runs slower, so consider reducing input size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a dictionary of inputs\n",
    "my_function_inputs = {\n",
    "    my_vector:[1,2,3],\n",
    "    my_vector2:[4,5,6]\n",
    "}\n",
    "\n",
    "# evaluate my_transformation\n",
    "# has to match with compiled function output\n",
    "print my_transformation.eval(my_function_inputs)\n",
    "\n",
    "\n",
    "# can compute transformations on the fly\n",
    "print \"add 2 vectors\", (my_vector + my_vector2).eval(my_function_inputs)\n",
    "\n",
    "#!WARNING! if your transformation only depends on some inputs,\n",
    "#do not provide the rest of them\n",
    "print \"vector's shape:\", my_vector.shape.eval({\n",
    "        my_vector:[1,2,3]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Для отладки желательно уменьшить масштаб задачи. Если вы планировали послать на вход вектор из 10^9 примеров, пошлите 10~100.\n",
    "* Если #ОЧЕНЬ нужно послать большой вектор, быстрее скомпилировать функцию обычным способом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь сам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Quest #1 - implement a function that computes a mean squared error of two input vectors\n",
    "# Your function has to take 2 vectors and return a single number\n",
    "\n",
    "<student.define_inputs_and_transformations()>\n",
    "\n",
    "compute_mse =<student.compile_function()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for n in [1,5,10,10**3]:\n",
    "    \n",
    "    elems = [np.arange(n),np.arange(n,0,-1), np.zeros(n),\n",
    "             np.ones(n),np.random.random(n),np.random.randint(100,size=n)]\n",
    "    \n",
    "    for el in elems:\n",
    "        for el_2 in elems:\n",
    "            true_mse = np.array(mean_squared_error(el,el_2))\n",
    "            my_mse = compute_mse(el,el_2)\n",
    "            if not np.allclose(true_mse,my_mse):\n",
    "                print 'Wrong result:'\n",
    "                print 'mse(%s,%s)'%(el,el_2)\n",
    "                print \"should be: %f, but your function returned %f\"%(true_mse,my_mse)\n",
    "                raise ValueError,\"Что-то не так\"\n",
    "\n",
    "print \"All tests passed\"\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared variables\n",
    "\n",
    "* The inputs and transformations only exist when function is called\n",
    "\n",
    "* Shared variables always stay in memory like global variables\n",
    " * Shared variables can be included into a symbolic graph\n",
    " * They can be set and evaluated using special methods\n",
    "   * but they can't change value arbitrarily during symbolic graph computation\n",
    "   * we'll cover that later;\n",
    " \n",
    " \n",
    "* Hint: such variables are a perfect place to store network parameters\n",
    " * e.g. weights or some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating shared variable\n",
    "shared_vector_1 = theano.shared(np.ones(10,dtype='float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial value [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#evaluating shared variable (outside symbolicd graph)\n",
    "print \"initial value\",shared_vector_1.get_value()\n",
    "\n",
    "# within symbolic graph you use them just as any other inout or transformation, not \"get value\" needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new value [ 0.  1.  2.  3.  4.]\n"
     ]
    }
   ],
   "source": [
    "#setting new value\n",
    "shared_vector_1.set_value( np.arange(5) )\n",
    "\n",
    "#getting that new value\n",
    "print \"new value\", shared_vector_1.get_value()\n",
    "\n",
    "#Note that the vector changed shape\n",
    "#This is entirely allowed... unless your graph is hard-wired to work with some fixed shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a recipe (transformation) that computes an elementwise transformation of shared_vector and input_scalar\n",
    "#Compile as a function of input_scalar\n",
    "\n",
    "input_scalar = T.scalar('coefficient',dtype='float32')\n",
    "\n",
    "scalar_times_shared = shared_vector_1*input_scalar\n",
    "\n",
    "\n",
    "#shared_times_n = <student.compile_function()>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"shared:\", shared_vector_1.get_value()\n",
    "\n",
    "print \"shared_times_n(5)\",shared_times_n(5)\n",
    "\n",
    "print \"shared_times_n(-0.5)\",shared_times_n(-0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Changing value of vector 1 (output should change)\n",
    "shared_vector_1.set_value([-1,0,1])\n",
    "print \"shared:\", shared_vector_1.get_value()\n",
    "\n",
    "print \"shared_times_n(5)\",shared_times_n(5)\n",
    "\n",
    "print \"shared_times_n(-0.5)\",shared_times_n(-0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T.grad - why theano matters\n",
    "* Theano can compute derivatives and gradients automatically\n",
    "* Derivatives are computed symbolically, not numerically\n",
    "\n",
    "Limitations:\n",
    "* You can only compute a gradient of a __scalar__ transformation over one or several scalar or vector (or tensor) transformations or inputs.\n",
    "* A transformation has to have float32 or float64 dtype throughout the whole computation graph\n",
    " * derivative over an integer has no mathematical sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_scalar = T.scalar(name='input',dtype='float64')\n",
    "\n",
    "scalar_squared = T.sum(my_scalar**2)\n",
    "\n",
    "#a derivative of v_squared by my_vector\n",
    "derivative = T.grad(scalar_squared,my_scalar)\n",
    "\n",
    "fun = theano.function([my_scalar],scalar_squared)\n",
    "grad = theano.function([my_scalar],derivative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7d5c2e0910>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEACAYAAACqOy3+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d61435890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "x = np.linspace(-3,3)\n",
    "x_squared = map(fun,x)\n",
    "x_squared_der = map(grad,x)\n",
    "\n",
    "plt.plot(x, x_squared,label=\"x^2\")\n",
    "plt.plot(x, x_squared_der, label=\"derivative\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why that rocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "my_vector = T.vector('float64')\n",
    "\n",
    "#Compute the gradient of the next weird function over my_scalar and my_vector\n",
    "#warning! Trying to understand the meaning of that function may result in permanent brain damage\n",
    "\n",
    "weird_psychotic_function = ((my_vector+my_scalar)**(1+T.var(my_vector)) +1./T.arcsinh(my_scalar)).mean()/(my_scalar**2 +1) + 0.01*T.sin(2*my_scalar**1.5)*(T.sum(my_vector)* my_scalar**2)*T.exp((my_scalar-4)**2)/(1+T.exp((my_scalar-4)**2))*(1.-(T.exp(-(my_scalar-4)**2))/(1+T.exp(-(my_scalar-4)**2)))**2\n",
    "\n",
    "\n",
    "der_by_scalar,der_by_vector = T.grad(weird_psychotic_function,[my_scalar,my_vector])\n",
    "\n",
    "\n",
    "compute_weird_function = theano.function([my_scalar,my_vector],weird_psychotic_function)\n",
    "compute_der_by_scalar = theano.function([my_scalar,my_vector],der_by_scalar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7d57e3f290>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEACAYAAACnJV25AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEIIIYQQQgghhBBCCCFc6/8DYoqna62mhAIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d5a2681d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting your derivative\n",
    "vector_0 = [1,2,3]\n",
    "\n",
    "scalar_space = np.linspace(0,7)\n",
    "\n",
    "y = [compute_weird_function(x,vector_0) for x in scalar_space]\n",
    "plt.plot(scalar_space,y,label='function')\n",
    "y_der_by_scalar = [compute_der_by_scalar(x,vector_0) for x in scalar_space]\n",
    "plt.plot(scalar_space,y_der_by_scalar,label='derivative')\n",
    "plt.grid();plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Almost done - Updates\n",
    "\n",
    "* updates are a way of changing shared variables at after function call.\n",
    "\n",
    "* technically it's a dictionary {shared_variable : a recipe for new value} which is has to be provided when function is compiled\n",
    "\n",
    "That's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multiply shared vector by a number and save the product back into shared vector\n",
    "\n",
    "inputs = [input_scalar]\n",
    "outputs = [scalar_times_shared] #return vector times scalar\n",
    "\n",
    "my_updates = {\n",
    "    shared_vector_1:scalar_times_shared #and write this same result bach into shared_vector_1\n",
    "}\n",
    "\n",
    "compute_and_save = theano.function(inputs, outputs, updates=my_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shared value: [ 0.  1.  2.  3.  4.]\n",
      "compute_and_save(2) returns [array([ 0.,  2.,  4.,  6.,  8.])]\n",
      "new shared value: [ 0.  2.  4.  6.  8.]\n"
     ]
    }
   ],
   "source": [
    "shared_vector_1.set_value(np.arange(5))\n",
    "\n",
    "#initial shared_vector_1\n",
    "print \"initial shared value:\" ,shared_vector_1.get_value()\n",
    "\n",
    "# evaluating the function (shared_vector_1 will be changed)\n",
    "print \"compute_and_save(2) returns\",compute_and_save(2)\n",
    "\n",
    "#evaluate new shared_vector_1\n",
    "print \"new shared value:\" ,shared_vector_1.get_value()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression example\n",
    "\n",
    "Implement the regular logistic regression training algorithm\n",
    "\n",
    "Tips:\n",
    "* Weights fit in as a shared variable\n",
    "* X and y are potential inputs\n",
    "* Compile 2 functions:\n",
    " * train_function(X,y) - returns error and computes weights' new values __(through updates)__\n",
    " * predict_fun(X) - just computes probabilities (\"y\") given data\n",
    " \n",
    " \n",
    "We shall train on a two-class MNIST dataset\n",
    "* please note that target y are {0,1} and not {-1,1} as in some formulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = load_digits(2)\n",
    "\n",
    "X,y = mnist.data, mnist.target\n",
    "\n",
    "\n",
    "print \"y [shape - %s]:\"%(str(y.shape)),y[:10]\n",
    "\n",
    "print \"X [shape - %s]:\"%(str(X.shape))\n",
    "print X[:3]\n",
    "print y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inputs and shareds\n",
    "shared_weights = <student.code_me()>\n",
    "input_X = <student.code_me()>\n",
    "input_y = <student.code_me()>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_y = <predicted probabilities for input_X>\n",
    "loss = <logistic loss (scalar, mean over sample)>\n",
    "\n",
    "grad = <gradient of loss over model weights>\n",
    "\n",
    "\n",
    "\n",
    "updates = {\n",
    "    shared_weights: <new weights after gradient step>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_function = <compile function that takes X and y, returns log loss and updates weights>\n",
    "predict_function = <compile function that takes X and computes probabilities of y>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for i in range(5):\n",
    "    loss_i = train_function(X_train,y_train)\n",
    "    print \"loss at iter %i:%.4f\"%(i,loss_i)\n",
    "    print \"train auc:\",roc_auc_score(y_train,predict_function(X_train))\n",
    "    print \"test auc:\",roc_auc_score(y_test,predict_function(X_test))\n",
    "\n",
    "    \n",
    "print \"resulting weights:\"\n",
    "plt.imshow(shared_weights.get_value().reshape(8,-1))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lasagne\n",
    "* lasagne is a library for neural network building and training\n",
    "* it's a low-level library with almost seamless integration with theano\n",
    "\n",
    "For a demo we shall solve the same digit recognition problem, but at a different scale\n",
    "* images are now 28x28\n",
    "* 10 different digits\n",
    "* 50k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz\n",
      "Downloading train-labels-idx1-ubyte.gz\n",
      "Downloading t10k-images-idx3-ubyte.gz\n",
      "Downloading t10k-labels-idx1-ubyte.gz\n",
      "(50000, 1, 28, 28) (50000,)\n"
     ]
    }
   ],
   "source": [
    "from mnist import load_dataset\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_dataset()\n",
    "\n",
    "print X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "\n",
    "input_X = T.tensor4(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = [None,1,28,28]\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Input layer (auxilary)\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "#fully connected layer, that takes input layer and applies 50 neurons to it.\n",
    "# nonlinearity here is sigmoid as in logistic regression\n",
    "# you can give a name to each layer (optional)\n",
    "dense_1 = lasagne.layers.DenseLayer(input_layer,num_units=50,\n",
    "                                   nonlinearity = lasagne.nonlinearities.sigmoid,\n",
    "                                   name = \"hidden_dense_layer\")\n",
    "\n",
    "#fully connected output layer that takes dense_1 as input and has 10 neurons (1 for each digit)\n",
    "#We use softmax nonlinearity to make probabilities add up to 1\n",
    "dense_output = lasagne.layers.DenseLayer(dense_1,num_units = 10,\n",
    "                                        nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#network prediction (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Softmax.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hidden_dense_layer.W, hidden_dense_layer.b, output.W, output.b]\n"
     ]
    }
   ],
   "source": [
    "#all network weights (shared variables)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output)\n",
    "print all_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Than you could simply\n",
    "* define loss function manually\n",
    "* compute error gradient over all weights\n",
    "* define updates\n",
    "* But that's a whole lot of work and life's short\n",
    "  * not to mention life's too short to wait for SGD to converge\n",
    "\n",
    "Instead, we shall use Lasagne builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.rmsprop(loss, all_weights,learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function that computes loss and updates weights\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#function that just computes accuracy\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's all, now let's train it!\n",
    "* We got a lot of data, so it's recommended that you use SGD\n",
    "* So let's implement a function that splits the training sample into minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An auxilary function that returns mini-batches for neural network training\n",
    "\n",
    "#Parameters\n",
    "# X - a tensor of images with shape (many, 1, 28, 28), e.g. X_train\n",
    "# y - a vector of answers for corresponding images e.g. Y_train\n",
    "#batch_size - a single number - the intended size of each batches\n",
    "\n",
    "#What do need to implement\n",
    "# 1) Shuffle data\n",
    "# - Gotta shuffle X and y the same way not to break the correspondence between X_i and y_i\n",
    "# 3) Split data into minibatches of batch_size\n",
    "# - If data size is not a multiple of batch_size, make one last batch smaller.\n",
    "# 4) return a list (or an iterator) of pairs\n",
    "# - (подгруппа картинок, ответы из y на эту подгруппу)\n",
    "def iterate_minibatches(X, y, batchsize):\n",
    "    \n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(X) - batchsize + 1, batchsize):\n",
    "        excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        yield X[excerpt], y[excerpt]        \n",
    "        \n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# You feel lost and wish you stayed home tonight?\n",
    "# Go search for a similar function at\n",
    "# https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 5.753s\n",
      "  training loss (in-iteration):\t\t0.286941\n",
      "  train accuracy:\t\t91.63 %\n",
      "  validation accuracy:\t\t95.61 %\n",
      "Epoch 2 of 100 took 5.257s\n",
      "  training loss (in-iteration):\t\t0.146217\n",
      "  train accuracy:\t\t95.61 %\n",
      "  validation accuracy:\t\t95.91 %\n",
      "Epoch 3 of 100 took 5.407s\n",
      "  training loss (in-iteration):\t\t0.112407\n",
      "  train accuracy:\t\t96.55 %\n",
      "  validation accuracy:\t\t96.43 %\n",
      "Epoch 4 of 100 took 5.292s\n",
      "  training loss (in-iteration):\t\t0.095250\n",
      "  train accuracy:\t\t97.07 %\n",
      "  validation accuracy:\t\t96.71 %\n",
      "Epoch 5 of 100 took 5.276s\n",
      "  training loss (in-iteration):\t\t0.083190\n",
      "  train accuracy:\t\t97.42 %\n",
      "  validation accuracy:\t\t96.69 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-a29a5096703c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mtrain_err_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_err_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 100 #amount of passes through the data\n",
    "\n",
    "batch_size = 50 #number of samples processed at each function call\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print \"Achievement unlocked: 80lvl Warlock!\"\n",
    "else:\n",
    "    print \"We need more magic!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A better network\n",
    "\n",
    "\n",
    "* The quest is to create a network that gets at least 99% at test set\n",
    " * In case you tried several architectures and have a __detailed__ report - 97.5% \"is fine too\". \n",
    " \n",
    "__ There is a mini-report at the end that you will have to fill in. We recommend to read it first and fill in while you are iterating. __\n",
    " \n",
    "\n",
    "## Tips on what can be done:\n",
    "\n",
    "\n",
    "\n",
    " * Network size\n",
    "   * MOAR neurons, \n",
    "   * MOAR layers, \n",
    "   * Пх'нглуи мглв'нафх Ктулху Р'льех вгах'нагл фхтагн! \n",
    "   \n",
    "   \n",
    "   \n",
    " * Regularize to prevent overfitting\n",
    "   * Add some L2 weight norm to the loss function, theano will do the rest\n",
    "   * Can be done manually or via - http://lasagne.readthedocs.org/en/latest/modules/regularization.html\n",
    "   \n",
    "   \n",
    "   \n",
    " * Better optimization - rmsprop, nesterov_momentum, adadelta, adagrad and so on.\n",
    "   * Converge faster and sometimes reach better optima\n",
    "   * It might make sense to tweak learning rate, other learning parameters, batch size and number of epochs\n",
    "   \n",
    "   \n",
    "   \n",
    " * Dropout - to prevent overfitting\n",
    "   * `lasagne.layers.DropoutLayer(prev_layer, p=probability_to_zero_out)`\n",
    "   \n",
    "   \n",
    "   \n",
    " * Convolution layers\n",
    "   * `network = lasagne.layers.Conv2DLayer(prev_layer,`\n",
    "    `                       num_filters = n_neurons,`\n",
    "    `                        filter_size = (filter width, filter height),`\n",
    "    `                        nonlinearity = some_nonlinearity)`\n",
    "   * Warning! Training convolutional networks can take long without GPU.\n",
    "     * If you are CPU-only, we still recomment to try a simple convolutional architecture\n",
    "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
    " \n",
    " * Plenty other layers and architectures\n",
    "   * http://lasagne.readthedocs.org/en/latest/modules/layers.html\n",
    "   * batch normalization, pooling, etc\n",
    "   \n",
    "   \n",
    " * Nonlinearities in the hidden layers\n",
    "   * tanh, relu, leaky relu, etc\n",
    "   \n",
    " \n",
    " \n",
    "   \n",
    "There is a template for your solution below that you can opt to use or throw away and write it your way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mnist import load_dataset\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_dataset()\n",
    "\n",
    "print X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "input_X = T.tensor4(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = [None,1,28,28]\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Input layer (auxilary)\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "<student.code_neural_network_architecture()>\n",
    "\n",
    "dense_output = <your network output>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network predictions (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All weights (shared-varaibles)\n",
    "# \"trainable\" flag means not to return auxilary params like batch mean (for batch normalization)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output,trainable=True)\n",
    "print all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss function\n",
    "loss = <loss function>\n",
    "\n",
    "#<optionally add regularization>\n",
    "\n",
    "accuracy = <mean accuracy score for evaluation> \n",
    "\n",
    "#weight updates\n",
    "updates = <try different update methods>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A function that accepts X and y, returns loss functions and performs weight updates\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#A function that just computes accuracy given X and y\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#итерации обучения\n",
    "\n",
    "num_epochs = <how many times to iterate over the entire training set>\n",
    "\n",
    "batch_size = <how many samples are processed at a single function call>\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print \"Achievement unlocked: 80lvl Warlock!\"\n",
    "else:\n",
    "    print \"We need more magic!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report\n",
    "\n",
    "All creative approaches are highly welcome, but at the very least it would be great to mention\n",
    "* the idea;\n",
    "* brief history of tweaks and improvements;\n",
    "* what is the final architecture and why?\n",
    "* what is the training method and, again, why?\n",
    "* Any regularizations and other techniques applied and their effects;\n",
    "\n",
    "\n",
    "There is no need to write strict mathematical proofs (unless you want to).\n",
    " * \"I tried this, this and this, and the second one turned out to be better. And i just didn't like the name of that one\" - OK, but can be better\n",
    " * \"I have analized these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\" - the ideal one\n",
    " * \"I took that code that demo without understanding it, but i'll never confess that and instead i'll make up some pseudoscientific explaination\" - __not_ok__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi, my name is `___ ___`, and here's my story\n",
    "\n",
    "A long ago in a galaxy far far away, when it was still more than an hour before deadline, i got an idea:\n",
    "\n",
    "##### I gonna build a neural network, that\n",
    "* brief text on what was\n",
    "* the original idea\n",
    "* and why it was so\n",
    "\n",
    "How could i be so naive?!\n",
    "\n",
    "##### One day, with no signs of warning,\n",
    "This thing has finally converged and\n",
    "* Some explaination about what were the results,\n",
    "* what worked and what didn't\n",
    "* most importantly - what next steps were taken, if any\n",
    "* and what were their respective outcomes\n",
    "\n",
    "##### Finally, after __  iterations, __ mugs of [tea/coffee]\n",
    "* what was the final architecture\n",
    "* as well as training method and tricks\n",
    "\n",
    "That, having wasted ____ [minutes, hours or days] of my life training, got\n",
    "\n",
    "* accuracy on training: __\n",
    "* accuracy on validation: __\n",
    "* accuracy on test: __\n",
    "\n",
    "\n",
    "[an optional afterword and mortal curses on assignment authors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
