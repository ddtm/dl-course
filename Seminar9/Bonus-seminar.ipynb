{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * 1-dimensional convolutions for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on job salary prediction\n",
    "![img](http://www.kdnuggets.com/images/cartoon-data-scientist-salary-negotiation.gif)\n",
    "\n",
    "Original conest - https://www.kaggle.com/c/job-salary-prediction\n",
    "\n",
    "\n",
    "### Download\n",
    "Go [here](https://www.kaggle.com/c/job-salary-prediction) and download as usual\n",
    "\n",
    "CSC cloud: data should already be here somewhere, just poke the nearest instructor.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Categorical fields - contract type, location\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Train_rev1.csv\",sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (244768, 12) 34122.5775755\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12612628</td>\n",
       "      <td>Engineering Systems Analyst</td>\n",
       "      <td>Engineering Systems Analyst Dorking Surrey Sal...</td>\n",
       "      <td>Dorking, Surrey, Surrey</td>\n",
       "      <td>Dorking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12612830</td>\n",
       "      <td>Stress Engineer Glasgow</td>\n",
       "      <td>Stress Engineer Glasgow Salary **** to **** We...</td>\n",
       "      <td>Glasgow, Scotland, Scotland</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 35000/annum 25-35K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12612844</td>\n",
       "      <td>Modelling and simulation analyst</td>\n",
       "      <td>Mathematical Modeller / Simulation Analyst / O...</td>\n",
       "      <td>Hampshire, South East, South East</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 40000/annum 20-40K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12613049</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 30000/annum 25K-30K negotiable</td>\n",
       "      <td>27500</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12613647</td>\n",
       "      <td>Pioneer, Miser Engineering Systems Analyst</td>\n",
       "      <td>Pioneer, Miser  Engineering Systems Analyst Do...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title  \\\n",
       "0  12612628                        Engineering Systems Analyst   \n",
       "1  12612830                            Stress Engineer Glasgow   \n",
       "2  12612844                   Modelling and simulation analyst   \n",
       "3  12613049  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  12613647         Pioneer, Miser Engineering Systems Analyst   \n",
       "\n",
       "                                     FullDescription  \\\n",
       "0  Engineering Systems Analyst Dorking Surrey Sal...   \n",
       "1  Stress Engineer Glasgow Salary **** to **** We...   \n",
       "2  Mathematical Modeller / Simulation Analyst / O...   \n",
       "3  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  Pioneer, Miser  Engineering Systems Analyst Do...   \n",
       "\n",
       "                         LocationRaw LocationNormalized ContractType  \\\n",
       "0            Dorking, Surrey, Surrey            Dorking          NaN   \n",
       "1        Glasgow, Scotland, Scotland            Glasgow          NaN   \n",
       "2  Hampshire, South East, South East          Hampshire          NaN   \n",
       "3     Surrey, South East, South East             Surrey          NaN   \n",
       "4     Surrey, South East, South East             Surrey          NaN   \n",
       "\n",
       "  ContractTime                       Company          Category  \\\n",
       "0    permanent  Gregory Martin International  Engineering Jobs   \n",
       "1    permanent  Gregory Martin International  Engineering Jobs   \n",
       "2    permanent  Gregory Martin International  Engineering Jobs   \n",
       "3    permanent  Gregory Martin International  Engineering Jobs   \n",
       "4    permanent  Gregory Martin International  Engineering Jobs   \n",
       "\n",
       "                                SalaryRaw  SalaryNormalized        SourceName  \n",
       "0              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  \n",
       "1              25000 - 35000/annum 25-35K             30000  cv-library.co.uk  \n",
       "2              20000 - 40000/annum 20-40K             30000  cv-library.co.uk  \n",
       "3  25000 - 30000/annum 25K-30K negotiable             27500  cv-library.co.uk  \n",
       "4              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.shape, df.SalaryNormalized.mean()\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.FullDescription.values,df.Title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.decode('utf8').lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEACAYAAABCl1qQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nugwLSVLX/wfq/6CuaebdzwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f985d2dddd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "_=plt.hist(token_counts.values(),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "\n",
    "min_count = 5\n",
    "tokens = <tokens from token_counts keys that had at least min_count occurences throughout the dataset>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 44867\n"
     ]
    }
   ],
   "source": [
    "print \"# Tokens:\",len(token_to_id)\n",
    "if len(token_to_id) < 10000:\n",
    "    print \"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\"\n",
    "if len(token_to_id) > 100000:\n",
    "    print \"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.decode('utf8').lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = map(lambda token: token_to_id.get(token,0), tokens)[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.FullDescription.values,token_to_id,max_len = 500)\n",
    "title_tokens = vectorize(df.Title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (244768, 15)\n",
      "Engineering Systems Analyst -> [38462 12311  1632     0     0     0     0     0     0     0] ...\n",
      "Stress Engineer Glasgow -> [19749 41620  5861     0     0     0     0     0     0     0] ...\n",
      "Modelling and simulation analyst -> [23387 16330 32144  1632     0     0     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print \"Matrix size:\",title_tokens.shape\n",
    "for title, tokens in zip(df.Title.values[:3],title_tokens[:3]):\n",
    "    print title,'->', tokens[:10],'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are categorical data. E.g. location, contract type, company\n",
    "\n",
    "They require a separate preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat = df[[\"Category\",\"LocationNormalized\",\"ContractType\",\"ContractTime\"]]\n",
    "\n",
    "\n",
    "categories = [A list of dictionaries {\"category\":category_name, \"subcategory\":subcategory_name} for each data sample]\n",
    "\n",
    "    \n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "df_non_text = vectorizer.fit_transform(categories)\n",
    "df_non_text = pd.DataFrame(df_non_text,columns=vectorizer.feature_names_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Split into training and test set.\n",
    "\n",
    "\n",
    "#Difficulty selector:\n",
    "#Easy: split randomly\n",
    "#Medium: split by companies, make sure no company is in both train and test set\n",
    "#Hard: do whatever you want, but score yourself using kaggle private leaderboard\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = <define_these_variables>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "save_prepared_data = True #save\n",
    "read_prepared_data = False #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "\n",
    "if save_prepared_data:\n",
    "    print \"Saving preprocessed data (may take up to 3 minutes)\"\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print \"done\"\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print \"Reading saved data...\"\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "\n",
    "\n",
    "        \n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "        \n",
    "    print \"done\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles\n",
    " * cnn+global max or RNN\n",
    "* Separate input for description\n",
    " * cnn+global max or RNN\n",
    "* Separate input for categorical features\n",
    " * Few dense layers + some black magic if you want\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple regression task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/Theano-0.8.0rc1-py2.7.egg/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.vector(\"is_blocked\",dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp,\n",
    "                                         input_size=len(token_to_id)+1,\n",
    "                                         output_size=?)\n",
    "\n",
    "\n",
    "#reshape from [batch, time, unit] to [batch,unit,time] to allow 1d convolution over time\n",
    "descr_nn = lasagne.layers.DimshuffleLayer(descr_nn, [0,2,1])\n",
    "\n",
    "descr_nn = 1D convolution over embedding, maybe several ones in a stack\n",
    "\n",
    "#pool over time\n",
    "descr_nn = lasagne.layers.GlobalPoolLayer(descr_nn,T.max)\n",
    "\n",
    "#Possible improvements here are adding several parallel convs with different filter sizes or stacking them the usual way\n",
    "#1dconv -> 1d max pool ->1dconv and finally global pool \n",
    "\n",
    "\n",
    "# Titles\n",
    "title_nn = <Process titles somehow (title_inp)>\n",
    "\n",
    "# Non-sequences\n",
    "cat_nn = <Process non-sequences(cat_inp)>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = <merge three layers into one (e.g. lasagne.layers.concat) >                                  \n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn,your_lucky_number)\n",
    "nn = lasagne.layers.DropoutLayer(nn,p=maybe_use_me)\n",
    "nn = lasagne.layers.DenseLayer(nn,1,nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#loss function\n",
    "loss = lasagne.objectives.squared_error(prediction,target_y).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = <your favorite optimizer>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn,deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = <an excercise in copy-pasting and editing>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    \n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump up and down for several epochs\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 100\n",
    "\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Train:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\trmse:',mean_squared_error(epoch_y_true,epoch_y_pred)**.5\n",
    "    print '\\tmae:',mean_absolute_error(epoch_y_true,epoch_y_pred)\n",
    "    \n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_ts,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Val:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\trmse:',mean_squared_error(epoch_y_true,epoch_y_pred)**.5\n",
    "    print '\\tmae:',mean_absolute_error(epoch_y_true,epoch_y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \n"
     ]
    }
   ],
   "source": [
    "print \"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_ts,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\trmse:',mean_squared_error(epoch_y_true,epoch_y_pred)**.5\n",
    "print '\\tmae:',mean_absolute_error(epoch_y_true,epoch_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now tune the monster for least MSE you can get!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next time in our show\n",
    "* Recurrent neural networks\n",
    " * How to apply them to practical problems?\n",
    " * What else can they do?\n",
    " * Why so much hype around LSTM?\n",
    "* Stay tuned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
